一、通用代码：
```
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

...

val spark = SparkSession.builder.appName("Simple Application").master("local").getOrCreate()
val sc = spark.sparkContext
import spark.implicits._
```


二、基础函数
describe 显示统计，functions下的 stddev，max
```
val ds = spark.range(0, 16)
val df1 = ds.select($"id", rand(57).alias("uniform distribution"), randn(15).alias("standard normal distribution"))
df1.show()
df1.describe("uniform distribution").show()
df1.select(stddev("uniform distribution"), max("uniform distribution")).show()
```

stat 方法获取 dataframe 的协方差等
```
val df2 = spark.range(0, 17)
    .withColumn("data1", rand(1290)).withColumn("data2", rand(5223))
//协方差(Covariance)
df2.stat.cov("id", "id")
//相关性(Correlation)
df2.stat.corr("data1", "data2")
```

根据字段查询，对查询的结果分组
```
val spyQueryResult = spyDF.select(year($"date").alias("year"), month($"date").alias("month"), $"adjClosePrice")
spyQueryResult.groupBy("year", "month")
      .avg("adjClosePrice").orderBy(desc("year"), desc("month"))
```

借助 sql 进行查询
```
xomDF.registerTempTable("xom")
apcDF.registerTempTable("apc")

val allDataQueryResult=spark.sql("SELECT apc.date, apc.adjClosePrice as apcClose, spy.adjClosePrice as spyClose, xom.adjClosePrice as xomClose from apc join spy on apc.date = spy.date join xom on spy.date = xom.date").cache()

val spyData = allDataQueryResult.select($"spyClose").map{row:Row => row.getAs[Double]("spyClose")}.rdd

val xomData = allDataQueryResult.select($"xomClose").map{row:Row => row.getAs[Double]("xomClose")}.rdd

val correlation = Statistics.corr(spyData, xomData)
```

udf 自定义函数1
```
val sfpdDF = sc.textFile("README.md")
  .map(line => line.split(" "))
  .map(s => Incidents(s(0), s(1), s(2), s(3), s(4)))
  .toDF

val getYearString = udf((s: String) => s.substring(s.lastIndexOf('/') + 1))
sfpdDF.groupBy(getYearString(sfpdDF("Date"))).count.show
```

udf 自定义函数2
```
val sfpdDF = sc.textFile("README.md")
  .map(line => line.split(" "))
  .map(s => Incidents(s(0), s(1), s(2), s(3), s(4)))
  .toDF

sfpdDF.createOrReplaceTempView("sfpd")

spark.udf.register("getYear", (s: String) => s.substring(s.lastIndexOf('/') + 1))

spark.sql("SELECT getYear(date), count(incidentnum) AS countbyyear FROM sfpd GROUP BY getYear(date) ORDER BY countbyyear DESC LIMIT 5").show
```
